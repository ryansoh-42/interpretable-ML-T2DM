{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25eb50c2",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "This notebook will focus on comparing the performance of models in 04_model_training.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3914164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open(\"../data/results/model_results.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "confusion_matrices = results[\"confusion_matrices\"]\n",
    "auc_scores = results[\"auc_scores\"]\n",
    "classification_reports = results[\"classification_reports\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ad94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Decision Tree:\n",
      "[[ 14   2  15]\n",
      " [  9 113  64]\n",
      " [  4   1   5]]\n",
      "Confusion Matrix for Random Forest:\n",
      "[[ 24   5   2]\n",
      " [ 12 155  19]\n",
      " [  3   4   3]]\n",
      "Confusion Matrix for XGBoost:\n",
      "[[ 21  10   0]\n",
      " [  9 177   0]\n",
      " [  3   7   0]]\n",
      "Confusion Matrix for AdaBoost:\n",
      "[[ 19   2  10]\n",
      " [  3 127  56]\n",
      " [  2   2   6]]\n",
      "Confusion Matrix for Logistic Regression:\n",
      "[[ 22   4   5]\n",
      " [  9 123  54]\n",
      " [  2   3   5]]\n",
      "Confusion Matrix for k-Nearest Neighbors:\n",
      "[[ 13  15   3]\n",
      " [  6 177   3]\n",
      " [  2   8   0]]\n",
      "Confusion Matrix for Support Vector Machine:\n",
      "[[ 12   6  13]\n",
      " [  2 163  21]\n",
      " [  2   5   3]]\n",
      "Confusion Matrix for Neural Network:\n",
      "[[ 22   4   5]\n",
      " [ 11 145  30]\n",
      " [  3   5   2]]\n"
     ]
    }
   ],
   "source": [
    "for model_name, confusion_matrix in confusion_matrices.items():\n",
    "    print(f\"Confusion Matrix for {model_name}:\\n{confusion_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c0ec1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results comparison:\n",
      "                        Macro Precision  Macro Recall  Macro F1  ROC AUC\n",
      "AdaBoost                         0.6148        0.6319    0.5462   0.8182\n",
      "Logistic Regression              0.5636        0.6237    0.5337   0.8170\n",
      "Neural Network                   0.5356        0.5631    0.5316   0.8085\n",
      "Random Forest                    0.5618        0.6358    0.5826   0.8040\n",
      "Support Vector Machine           0.5893        0.5211    0.5146   0.7976\n",
      "XGBoost                          0.5162        0.5430    0.5293   0.7961\n",
      "Decision Tree                    0.5174        0.5197    0.4458   0.7596\n",
      "k-Nearest Neighbors              0.5013        0.4570    0.4724   0.7459\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the models and combine the macro precision, recall, f1-score, and ROC AUC into a single DataFrame\n",
    "results_comparison = {}\n",
    "\n",
    "for model_name in classification_reports.keys():\n",
    "    classification_report = classification_reports[model_name]\n",
    "    auc_score = auc_scores[model_name]\n",
    "    \n",
    "    results_comparison[model_name] = {\n",
    "        \"Macro Precision\" : classification_report[\"macro avg\"][\"precision\"],\n",
    "        \"Macro Recall\" : classification_report[\"macro avg\"][\"recall\"],\n",
    "        \"Macro F1\" : classification_report[\"macro avg\"][\"f1-score\"],\n",
    "        \"ROC AUC\" : auc_scores[model_name]\n",
    "    }\n",
    "    \n",
    "results_comparison_df = pd.DataFrame.from_dict(results_comparison, orient=\"index\")\n",
    "results_comparison_df = results_comparison_df.round(4).sort_values(\"ROC AUC\", ascending=False)\n",
    "\n",
    "print(f\"Model results comparison:\\n{results_comparison_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "032b1ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC comparison:\n",
      "                        ROC AUC\n",
      "AdaBoost                 0.8182\n",
      "Logistic Regression      0.8170\n",
      "Neural Network           0.8085\n",
      "Random Forest            0.8040\n",
      "Support Vector Machine   0.7976\n",
      "XGBoost                  0.7961\n",
      "Decision Tree            0.7596\n",
      "k-Nearest Neighbors      0.7459\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the models and compare the ROC AUC scores\n",
    "auc_comparison = {}\n",
    "\n",
    "for model_name, auc_score in auc_scores.items():\n",
    "    \n",
    "    auc_comparison[model_name] = {\n",
    "        \"ROC AUC\" : auc_score\n",
    "    }\n",
    "    \n",
    "    auc_comparisonn_df = pd.DataFrame.from_dict(auc_comparison, orient=\"index\")\n",
    "    \n",
    "auc_comparisonn_df = auc_comparisonn_df.round(4).sort_values(\"ROC AUC\", ascending=False)\n",
    "\n",
    "print(f\"ROC AUC comparison:\\n{auc_comparisonn_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d63d53e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diabetes_yes] Recall comparison:\n",
      "                        Recall\n",
      "Random Forest           0.7742\n",
      "Logistic Regression     0.7097\n",
      "Neural Network          0.7097\n",
      "XGBoost                 0.6774\n",
      "AdaBoost                0.6129\n",
      "Decision Tree           0.4516\n",
      "k-Nearest Neighbors     0.4194\n",
      "Support Vector Machine  0.3871\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the models and compare the recall scores of \"Diabetes_yes\"\n",
    "yes_recall_comparison = {}\n",
    "\n",
    "for model_name, classification_report in classification_reports.items():\n",
    "    \n",
    "    yes_recall_comparison[model_name] = {\n",
    "        \"Recall\" : classification_report[\"Diabetes_yes\"][\"recall\"]\n",
    "    }\n",
    "    \n",
    "    yes_recall_comparison_df = pd.DataFrame.from_dict(yes_recall_comparison, orient=\"index\")\n",
    "    \n",
    "yes_recall_comparison_df = yes_recall_comparison_df.round(4).sort_values(\"Recall\", ascending=False)\n",
    "\n",
    "print(f\"[Diabetes_yes] Recall comparison:\\n{yes_recall_comparison_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23798237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diabetes_no] Precision comparison:\n",
      "                        Precision\n",
      "Decision Tree              0.9741\n",
      "AdaBoost                   0.9695\n",
      "Logistic Regression        0.9462\n",
      "Random Forest              0.9451\n",
      "Neural Network             0.9416\n",
      "Support Vector Machine     0.9368\n",
      "XGBoost                    0.9124\n",
      "k-Nearest Neighbors        0.8850\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the models and compare the precision scores of \"Diabetes_no\"\n",
    "no_precision_comparison = {}\n",
    "\n",
    "for model_name, classification_report in classification_reports.items():\n",
    "    \n",
    "    no_precision_comparison[model_name] = {\n",
    "        \"Precision\" : classification_report[\"Diabetes_no\"][\"precision\"]\n",
    "    }\n",
    "    \n",
    "    no_precision_comparison_df = pd.DataFrame.from_dict(no_precision_comparison, orient=\"index\")\n",
    "    \n",
    "no_precision_comparison_df = no_precision_comparison_df.round(4).sort_values(\"Precision\", ascending=False)\n",
    "\n",
    "print(f\"[Diabetes_no] Precision comparison:\\n{no_precision_comparison_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc0b59e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Diabetes_borderline] Recall comparison:\n",
      "                        Recall\n",
      "AdaBoost                   0.6\n",
      "Decision Tree              0.5\n",
      "Logistic Regression        0.5\n",
      "Random Forest              0.3\n",
      "Support Vector Machine     0.3\n",
      "Neural Network             0.2\n",
      "XGBoost                    0.0\n",
      "k-Nearest Neighbors        0.0\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the models and compare the recall scores of \"Diabetes_borderline\"\n",
    "borderline_recall_comparison = {}\n",
    "\n",
    "for model_name, classification_report in classification_reports.items():\n",
    "    \n",
    "    borderline_recall_comparison[model_name] = {\n",
    "        \"Recall\" : classification_report[\"Diabetes_borderline\"][\"recall\"]\n",
    "    }\n",
    "    \n",
    "    borderline_recall_comparison_df = pd.DataFrame.from_dict(borderline_recall_comparison, orient=\"index\")\n",
    "    \n",
    "borderline_recall_comparison_df = borderline_recall_comparison_df.round(4).sort_values(\"Recall\", ascending=False)\n",
    "\n",
    "print(f\"[Diabetes_borderline] Recall comparison:\\n{borderline_recall_comparison_df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc783ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
